

## üö¶ WIP üö¶

## ü¶í Colab
| colab | Info - Model Page
| --- | --- |
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/camenduru/text-generation-webui-colab/blob/main/vicuna-13b-GPTQ-4bit-128g.ipynb) | vicuna-13b-GPTQ-4bit-128g <br /> https://vicuna.lmsys.org
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/camenduru/text-generation-webui-colab/blob/main/vicuna-13B-1.1-GPTQ-4bit-128g.ipynb) | vicuna-13B-1.1-GPTQ-4bit-128g <br /> https://vicuna.lmsys.org
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/camenduru/text-generation-webui-colab/blob/main/stable-vicuna-13B-GPTQ-4bit-128g.ipynb) | stable-vicuna-13B-GPTQ-4bit-128g <br /> https://huggingface.co/CarperAI/stable-vicuna-13b-delta
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/camenduru/text-generation-webui-colab/blob/main/gpt4-x-alpaca-13b-native-4bit-128g.ipynb) | gpt4-x-alpaca-13b-native-4bit-128g <br /> https://huggingface.co/chavinlo/gpt4-x-alpaca
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/camenduru/text-generation-webui-colab/blob/main/pyg-7b-GPTQ-4bit-128g.ipynb) | pyg-7b-GPTQ-4bit-128g <br /> https://huggingface.co/Neko-Institute-of-Science/pygmalion-7b
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/camenduru/text-generation-webui-colab/blob/main/koala-13B-GPTQ-4bit-128g.ipynb) | koala-13B-GPTQ-4bit-128g <br /> https://bair.berkeley.edu/blog/2023/04/03/koala
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/camenduru/text-generation-webui-colab/blob/main/oasst-llama13b-GPTQ-4bit-128g.ipynb) | oasst-llama13b-GPTQ-4bit-128g <br /> https://open-assistant.io
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/camenduru/text-generation-webui-colab/blob/main/wizard-lm-uncensored-7b-GPTQ-4bit-128g.ipynb) | wizard-lm-uncensored-7b-GPTQ-4bit-128g <br /> https://github.com/nlpxucan/WizardLM
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/camenduru/text-generation-webui-colab/blob/main/mpt-storywriter-7b-GPTQ-4bit-128g.ipynb) | mpt-storywriter-7b-GPTQ-4bit-128g <br /> https://www.mosaicml.com
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/camenduru/text-generation-webui-colab/blob/main/wizard-lm-uncensored-13b-GPTQ-4bit-128g.ipynb) | wizard-lm-uncensored-13b-GPTQ-4bit-128g <br /> https://github.com/nlpxucan/WizardLM
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/camenduru/text-generation-webui-colab/blob/main/pyg-13b-GPTQ-4bit-128g.ipynb) | pyg-13b-GPTQ-4bit-128g <br /> https://huggingface.co/PygmalionAI/pygmalion-13b
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/camenduru/text-generation-webui-colab/blob/main/falcon-7b-instruct-GPTQ-4bit.ipynb) | falcon-7b-instruct-GPTQ-4bit <br /> https://falconllm.tii.ae/
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/camenduru/text-generation-webui-colab/blob/main/wizard-lm-13b-1.1-GPTQ-4bit-128g.ipynb) | wizard-lm-13b-1.1-GPTQ-4bit-128g <br /> https://github.com/nlpxucan/WizardLM
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/camenduru/text-generation-webui-colab/blob/main/llama-2-7b-chat-GPTQ-4bit.ipynb) | llama-2-7b-chat-GPTQ-4bit (4bit) <br /> https://ai.meta.com/llama/
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/camenduru/text-generation-webui-colab/blob/main/llama-2-13b-chat-GPTQ-4bit.ipynb) | llama-2-13b-chat-GPTQ-4bit (4bit) <br /> https://ai.meta.com/llama/ <br /> üö¶ WIP üö¶ please try llama-2-13b-chat or llama-2-7b-chat or llama-2-7b-chat-GPTQ-4bit
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/camenduru/text-generation-webui-colab/blob/main/llama-2-7b-chat.ipynb) | llama-2-7b-chat (16bit) <br /> https://ai.meta.com/llama/
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/camenduru/text-generation-webui-colab/blob/main/llama-2-13b-chat.ipynb) | llama-2-13b-chat (8bit) <br /> https://ai.meta.com/llama/
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/camenduru/text-generation-webui-colab/blob/main/redmond-puffin-13b-GPTQ-4bit.ipynb) | redmond-puffin-13b-GPTQ-4bit (4bit) <br /> https://huggingface.co/NousResearch/Redmond-Puffin-13B
<!-- [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/camenduru/text-generation-webui-colab/blob/main/nous-hermes-13b-GPTQ-4bit.ipynb) | nous-hermes-13b-GPTQ-4bit (4bit) <br /> https://huggingface.co/NousResearch/Nous-Hermes-Llama2-13b -->

## ü¶í Colab Pro
According to the Facebook Research LLaMA license (Non-commercial bespoke license), maybe we cannot use this model with a Colab Pro account.
But Yann LeCun said "GPL v3" (https://twitter.com/ylecun/status/1629189925089296386) I am a little confused. Is it possible to use this with a non-free Colab Pro account?

## Tutorial
https://www.youtube.com/watch?v=kgA7eKU1XuA

## Text Generation Web UI
[https://github.com/oobabooga/text-generation-webui](https://github.com/oobabooga/text-generation-webui) (Thanks to @oobabooga ‚ù§)

## Models License
| Model | License
| --- | --- |
vicuna-13b-GPTQ-4bit-128g | From https://vicuna.lmsys.org: The online demo is a research preview intended for non-commercial use only, subject to the model [License](https://github.com/facebookresearch/llama/blob/main/MODEL_CARD.md) of LLaMA, Terms of Use of the data generated by OpenAI, and Privacy Practices of ShareGPT. Please contact us If you find any potential violation. The code is released under the Apache License 2.0.
gpt4-x-alpaca-13b-native-4bit-128g | https://huggingface.co/chavinlo/alpaca-native -> https://huggingface.co/chavinlo/alpaca-13b -> https://huggingface.co/chavinlo/gpt4-x-alpaca
llama-2 | https://ai.meta.com/llama/ Llama 2 is available for free for research and commercial use. ü•≥

## Special Thanks
Thanks to facebookresearch ‚ù§ for https://github.com/facebookresearch/llama <br />
Thanks to lmsys ‚ù§ for https://huggingface.co/lmsys/vicuna-13b-delta-v0 <br />
Thanks to anon8231489123 ‚ù§ for https://huggingface.co/anon8231489123/vicuna-13b-GPTQ-4bit-128g (GPTQ 4bit quantization of: https://huggingface.co/lmsys/vicuna-13b-delta-v0) <br />
Thanks to tatsu-lab ‚ù§ for https://github.com/tatsu-lab/stanford_alpaca <br />
Thanks to chavinlo ‚ù§ for https://huggingface.co/chavinlo/gpt4-x-alpaca <br />
Thanks to qwopqwop200 ‚ù§ for https://github.com/qwopqwop200/GPTQ-for-LLaMa <br />
Thanks to tsumeone ‚ù§ for https://huggingface.co/tsumeone/gpt4-x-alpaca-13b-native-4bit-128g-cuda (GPTQ 4bit quantization of: https://huggingface.co/chavinlo/gpt4-x-alpaca) <br />
Thanks to transformers ‚ù§ for https://github.com/huggingface/transformers <br />
Thanks to gradio-app ‚ù§ for https://github.com/gradio-app/gradio <br />
Thanks to TheBloke ‚ù§ for https://huggingface.co/TheBloke/stable-vicuna-13B-GPTQ <br />
Thanks to Neko-Institute-of-Science ‚ù§ for https://huggingface.co/Neko-Institute-of-Science/pygmalion-7b <br />
Thanks to gozfarb ‚ù§ for https://huggingface.co/gozfarb/pygmalion-7b-4bit-128g-cuda (GPTQ 4bit quantization of: https://huggingface.co/Neko-Institute-of-Science/pygmalion-7b) <br />
Thanks to young-geng ‚ù§ for https://huggingface.co/young-geng/koala <br />
Thanks to TheBloke ‚ù§ for https://huggingface.co/TheBloke/koala-13B-GPTQ-4bit-128g (GPTQ 4bit quantization of: https://huggingface.co/young-geng/koala) <br />
Thanks to dvruette ‚ù§ for https://huggingface.co/dvruette/oasst-llama-13b-2-epochs <br />
Thanks to gozfarb ‚ù§ for https://huggingface.co/gozfarb/oasst-llama13b-4bit-128g (GPTQ 4bit quantization of: https://huggingface.co/dvruette/oasst-llama-13b-2-epochs) <br />
Thanks to ehartford ‚ù§ for https://huggingface.co/ehartford/WizardLM-7B-Uncensored <br />
Thanks to TheBloke ‚ù§ for https://huggingface.co/TheBloke/WizardLM-7B-uncensored-GPTQ (GPTQ 4bit quantization of: https://huggingface.co/ehartford/WizardLM-7B-Uncensored) <br />
Thanks to mosaicml ‚ù§ for https://huggingface.co/mosaicml/mpt-7b-storywriter <br />
Thanks to OccamRazor ‚ù§ for https://huggingface.co/OccamRazor/mpt-7b-storywriter-4bit-128g (GPTQ 4bit quantization of: https://huggingface.co/mosaicml/mpt-7b-storywriter) <br />
Thanks to ehartford ‚ù§ for https://huggingface.co/ehartford/WizardLM-13B-Uncensored <br />
Thanks to ausboss ‚ù§ for https://huggingface.co/ausboss/WizardLM-13B-Uncensored-4bit-128g (GPTQ 4bit quantization of: https://huggingface.co/ehartford/WizardLM-13B-Uncensored) <br />
Thanks to PygmalionAI ‚ù§ for https://huggingface.co/PygmalionAI/pygmalion-13b <br />
Thanks to notstoic ‚ù§ for https://huggingface.co/notstoic/pygmalion-13b-4bit-128g (GPTQ 4bit quantization of: https://huggingface.co/PygmalionAI/pygmalion-13b) <br />
Thanks to WizardLM ‚ù§ for https://huggingface.co/WizardLM/WizardLM-13B-V1.1 <br />
Thanks to TheBloke ‚ù§ for https://huggingface.co/TheBloke/WizardLM-13B-V1.1-GPTQ (GPTQ 4bit quantization of: https://huggingface.co/WizardLM/WizardLM-13B-V1.1) <br />
Thanks to meta-llama ‚ù§ for https://huggingface.co/meta-llama/Llama-2-7b-chat-hf <br />
Thanks to TheBloke ‚ù§ for https://huggingface.co/TheBloke/Llama-2-7b-Chat-GPTQ (GPTQ 4bit quantization of: https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) <br />
Thanks to meta-llama ‚ù§ for https://huggingface.co/meta-llama/Llama-2-13b-chat-hf <br />
Thanks to localmodels ‚ù§ for https://huggingface.co/localmodels/Llama-2-13B-Chat-GPTQ (GPTQ 4bit quantization of: https://huggingface.co/meta-llama/Llama-2-13b-chat-hf) <br />
Thanks to NousResearch ‚ù§ for https://huggingface.co/NousResearch/Redmond-Puffin-13B <br />
Thanks to TheBloke ‚ù§ for https://huggingface.co/TheBloke/Redmond-Puffin-13B-GPTQ (GPTQ 4bit quantization of: https://huggingface.co/NousResearch/Redmond-Puffin-13B) <br />
<!-- Thanks to NousResearch ‚ù§ for https://huggingface.co/NousResearch/Nous-Hermes-Llama2-13b (GPTQ 4bit quantization https://huggingface.co/NousResearch/Nous-Hermes-Llama2-13b-GPTQ)<br />  -->
